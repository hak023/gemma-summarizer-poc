import sys
import os
import time
import traceback
import threading
import re
import json
import multiprocessing
from pathlib import Path
from config import get_config, get_model_path, validate_config
from logger import log_gemma_query, log_gemma_response
from preprocessor import STTPreprocessor
from postprocessor import ResponsePostprocessor
from llm_utils import correct_conversation_with_gemma
from json_repair import (
    extract_json_from_markdown,
    process_and_repair_json,
    extract_valid_data_from_broken_json
)

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_llm_instance = None
_llm_lock = threading.Lock()

def resource_path(relative_path):
    """PyInstaller í™˜ê²½ì—ì„œ ë¦¬ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ë°˜í™˜"""
    try:
        # PyInstaller í™˜ê²½ì—ì„œëŠ” _MEIPASS ê²½ë¡œ ì‚¬ìš©
        if hasattr(sys, '_MEIPASS'):
            base_path = sys._MEIPASS
        else:
            base_path = os.path.abspath(".")

        full_path = os.path.join(base_path, relative_path)
        print(f"ëª¨ë¸ ê²½ë¡œ: {full_path}")
        print(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(full_path)}")
        return full_path
    except Exception as e:
        print(f"ë¦¬ì†ŒìŠ¤ ê²½ë¡œ ì˜¤ë¥˜: {e}")
        return relative_path


def get_llm_instance():
    """ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _llm_instance

    if _llm_instance is None:
        with _llm_lock:
            if _llm_instance is None:  # Double-checked locking
                try:
                    print("llama_cpp ëª¨ë“ˆ ì„í¬íŠ¸ ì¤‘...")
                    from llama_cpp import Llama
                    print("llama_cpp ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")

                    # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                    config = get_config()
                    MODEL_PATH = get_model_path()

                    print(f"ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_PATH}")
                    
                    # CPU ì œí•œ ê°•ì œ ì ìš©
                    force_threads = config.get('MAX_CPU_THREADS')
                    if force_threads is not None:
                        # ê°•ì œ ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì •
                        max_threads = force_threads
                        print(f"ê°•ì œ ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì •: {max_threads}")
                    else:
                        # get_optimal_threads() ê²°ê³¼ ì‚¬ìš©
                        from config import get_optimal_threads
                        max_threads = get_optimal_threads()
                    
                    print(f"ìµœì¢… ì‚¬ìš© ìŠ¤ë ˆë“œ ìˆ˜: {max_threads}")
                    
                    # GPU ì‚¬ìš© ì„¤ì • ì ìš©
                    enable_gpu = bool(config.get('ENABLE_GPU', False))
                    n_gpu_layers = 0
                    env_n_gpu_layers = os.getenv('N_GPU_LAYERS')
                    if enable_gpu:
                        try:
                            n_gpu_layers = int(env_n_gpu_layers) if env_n_gpu_layers is not None else -1
                        except ValueError:
                            n_gpu_layers = -1
                    # CUDA/ì˜¤í”„ë¡œë”© ì§€ì› ë° í™˜ê²½ ì •ë³´ ì¶œë ¥
                    try:
                        import llama_cpp as _llama_cpp
                        from llama_cpp import llama_supports_gpu_offload as _llama_supports_gpu_offload
                        print(f"llama_cpp ë²„ì „: {getattr(_llama_cpp, '__version__', 'n/a')}")
                        print(f"GPU ì˜¤í”„ë¡œë”© ì§€ì›: {_llama_supports_gpu_offload()}")
                    except Exception as _gpu_info_err:
                        print(f"GPU ì§€ì› ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {_gpu_info_err}")
                    print(f"CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES')}")
                    print(f"GPU ì‚¬ìš© ì„¤ì •: {'í™œì„±í™”' if enable_gpu else 'ë¹„í™œì„±í™”'} (n_gpu_layers={n_gpu_layers})")

                    # OS ë ˆë²¨ì—ì„œ CPU ì‚¬ìš©ëŸ‰ ì œí•œ ì„¤ì •
                    if hasattr(os, 'sched_setaffinity'):
                        # Linuxì—ì„œ CPU ì½”ì–´ ì œí•œ
                        available_cpus = list(range(max_threads))
                        os.sched_setaffinity(0, available_cpus)
                        print(f"CPU ì¹œí™”ì„± ì„¤ì •: {available_cpus}")
                    elif os.name == 'nt':
                        # Windowsì—ì„œ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì¡°ì •
                        try:
                            import psutil
                            current_process = psutil.Process()
                            current_process.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
                            print(f"í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì¡°ì • ì™„ë£Œ")
                        except ImportError:
                            print("psutil íŒ¨í‚¤ì§€ê°€ ì—†ì–´ ìš°ì„ ìˆœìœ„ ì¡°ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                    
                    # í™˜ê²½ë³€ìˆ˜ë¡œ OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
                    os.environ['OMP_NUM_THREADS'] = str(max_threads)
                    os.environ['MKL_NUM_THREADS'] = str(max_threads)
                    os.environ['OPENBLAS_NUM_THREADS'] = str(max_threads)
                    os.environ['VECLIB_MAXIMUM_THREADS'] = str(max_threads)
                    os.environ['NUMEXPR_NUM_THREADS'] = str(max_threads)
                    
                    print(f"í™˜ê²½ë³€ìˆ˜ ìŠ¤ë ˆë“œ ì œí•œ ì„¤ì •: {max_threads}")

                    _llm_instance = Llama(
                        model_path=MODEL_PATH,
                        n_ctx=config['MODEL_CONTEXT_SIZE'],
                        n_threads=max_threads,
                        n_threads_batch=max_threads,  # ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤ë ˆë“œë„ ì œí•œ
                        n_gpu_layers=n_gpu_layers,
                        verbose=False  # ë¶ˆí•„ìš”í•œ ì¶œë ¥ ì¤„ì´ê¸°
                    )
                    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

                except Exception as e:
                    print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    raise

    return _llm_instance

def summarize_with_gemma(text: str, max_tokens: int = None) -> str:
    """
    Gemma ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

    Args:
        text (str): ìš”ì•½í•  í…ìŠ¤íŠ¸
        max_tokens (int, optional): ìµœëŒ€ í† í° ìˆ˜. Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©

    Returns:
        str: ë°˜ë“œì‹œ JSON í˜•íƒœì˜ ë¬¸ìì—´ (summary í‚¤ì— ìš”ì•½)
    """
    try:
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        config = get_config()
        if max_tokens is None:
            max_tokens = config['DEFAULT_MAX_TOKENS']

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)
        if text and isinstance(text, str):
            # ëŒ€í™” í˜•íƒœë¡œ ë¶„ë¦¬
            lines = text.strip().split('\n')
            # ì¤‘ë³µ ì œê±°
            cleaned_lines = STTPreprocessor.remove_duplicates(lines)
            # ë‹¤ì‹œ ê²°í•©
            text = '\n'.join(cleaned_lines)
            print(f"ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")

        llm = get_llm_instance()

        # í”„ë¡¬í”„íŠ¸ë¥¼ ìš”ì  ì¤‘ì‹¬ìœ¼ë¡œ ë³€ê²½ (ê°„ê²°í•œ ìš”ì•½) - ê°•ì œì„± ê°•í™”
        prompt = (
            f"ë‹¹ì‹ ì€ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
            f"ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\n"
            #f"ì•„ë˜ [ë¶„ì„ ê·œì¹™]ì„ ì°¸ê³ í•˜ì—¬, [ì›ë³¸ í†µí™” ë‚´ìš©]ì„ ë¶„ì„í•˜ê³  ì™„ë²½í•œ JSONì„ ìƒì„±í•˜ì„¸ìš”.\n\n"
            f"--- [ë¶„ì„ ê·œì¹™] ---\n"
            f"summary: í†µí™”ì˜ í•µì‹¬ ë‚´ìš©ì„ 25ì ì´ë‚´ì˜ ì£¼ì–´ë¥¼ ì œì™¸í•œ ë§¤ìš° ì§§ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. ë¬¸ì¥ì˜ ëì€ 'ëª…ì‚¬í˜•' ìœ¼ë¡œ ëë‚´ì•¼ í•©ë‹ˆë‹¤.\n"
            f"keyword: ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ 3ê°œ ì¶”ì¶œí•˜ì—¬ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì„¸ìš”.\n"
            f"paragraphs: í†µí™” ë‚´ìš©ì„ ë°˜ë“œì‹œ 2-3ê°œì˜ ë…¼ë¦¬ì  ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ê°ê° ë¶„ì„í•˜ì„¸ìš”.\n"
            f"  - ê° paragraphëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:\n"
            f"    * summary: í•´ë‹¹ ë¶€ë¶„ì˜ í•µì‹¬ ë‚´ìš©ì„ 25ì ì´ë‚´ë¡œ ìš”ì•½\n"
            f"    * keyword: í•´ë‹¹ ë¶€ë¶„ì˜ ì£¼ìš” í‚¤ì›Œë“œ 3ê°œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„\n"
            f"    * sentiment: ê°ì •ì„ 'ê°•í•œê¸ì •', 'ì•½í•œê¸ì •', 'ë³´í†µ', 'ì•½í•œë¶€ì •', 'ê°•í•œë¶€ì •' ì¤‘ì—ì„œ ì„ íƒ\n\n"
            f"--- [ì‘ë‹µ í˜•ì‹] ---\n"
            f"ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:\n"
            f"```json\n"
            f'{{\n'
            f'"summary": "í†µí™” í•µì‹¬ ìš”ì•½",\n'
            f'"keyword": "",\n'
            f'"paragraphs": [\n'
            f'{{\n'
            f'"summary": "",\n'
            f'"keyword": "",\n'
            f'"sentiment": ""\n'
            f'}},\n'
            f'{{\n'
            f'"summary": "",\n'
            f'"keyword": "",\n'
            f'"sentiment": ""\n'
            f'}}\n'
            f']\n'
            f'}}\n'
            f"```\n\n"
            f"ëŒ€í™” ë‚´ìš©:\n{text}\n\n"
            f"ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë°˜ë“œì‹œ paragraphsë¥¼ í¬í•¨í•œ ì™„ì „í•œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
        )

        print("ìš”ì•½ ìƒì„± ì¤‘...")
        log_gemma_query(prompt, "gemma_summarizer")

        # Gemma Query ì‹œê°„ ì¸¡ì • ì‹œì‘
        gemma_query_start = time.time()
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©
        config = get_config()
        model_timeout = config.get('MODEL_TIMEOUT', 180.0)
        
        # í† í° ìˆ˜ ì„¤ì • (Context Window ê³ ë ¤í•˜ì—¬ ë™ì  ê³„ì‚°)
        config = get_config()
        context_size = config['MODEL_CONTEXT_SIZE']
        
        # í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œ í† í°í™”ëŠ” ë¹„ìš©ì´ í¬ë¯€ë¡œ ì¶”ì •)
        # ì™„ì„±ëœ prompt ê¸¸ì´ë¥¼ ì§ì ‘ ì‚¬ìš© (í•œê¸€ 1ê¸€ì â‰ˆ 0.8í† í°)
        estimated_prompt_tokens = len(prompt) * 0.8  # í•œê¸€ í† í° ë¹„ìœ¨ ì ìš© (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
        available_tokens = context_size - estimated_prompt_tokens - 100  # 100í† í° ì—¬ìœ 
        
        # max_tokensë¥¼ ì‚¬ìš© ê°€ëŠ¥í•œ í† í° ìˆ˜ë¡œ ì œí•œ (ìµœì†Œê°’ ë³´ì¥)
        max_tokens = max(500, min(4000, available_tokens))  # ìµœì†Œ 500, ìµœëŒ€ 4000í† í°
        
        # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ Context Window ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
        if available_tokens < 500:
            print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ Context Windowë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!")
            print(f"Context: {context_size}, í”„ë¡¬í”„íŠ¸: {estimated_prompt_tokens}")
            print(f"í…ìŠ¤íŠ¸ë¥¼ ì¤„ì´ê±°ë‚˜ Context Windowë¥¼ ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.")
            max_tokens = 500  # ìµœì†Œ ì‘ë‹µ ë³´ì¥
        
        print(f"ì¶”ì • í”„ë¡¬í”„íŠ¸ í† í°: {estimated_prompt_tokens}, ì‚¬ìš© ê°€ëŠ¥ í† í°: {available_tokens}, ì„¤ì •ëœ max_tokens: {max_tokens}")
        
        print(f"ëª¨ë¸ ì¶”ë¡  ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {model_timeout}ì´ˆ)")
        
        # 1B 8Q ëª¨ë¸ì— ë§ëŠ” íŒŒë¼ë¯¸í„° ì¡°ì • (ì¼ê´€ì„± ê°•í™”)
        print(f"ì„¤ì •ëœ max_tokens: {max_tokens}")
        output = llm(
            prompt,
            max_tokens=max_tokens,
            temperature=0.3,  # ë§¤ìš° ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± ê·¹ëŒ€í™”
            min_p=0.1,  # ë” ì—„ê²©í•œ ìµœì†Œ í™•ë¥ 
            top_p=0.8,  # ë” ë‚®ì€ top_pë¡œ ì¼ê´€ì„± í–¥ìƒ
            top_k=20,  # ë” ì¢ì€ í† í° ì„ íƒ ë²”ìœ„
            repeat_penalty=1.05,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
            echo=False
        )
        
        # Gemma Query ì‹œê°„ ì¸¡ì • ì™„ë£Œ
        gemma_query_end = time.time()
        gemma_query_elapsed = gemma_query_end - gemma_query_start
        print(f"[Gemma Query ì†Œìš”ì‹œê°„] {gemma_query_elapsed:.2f}ì´ˆ")
        
        print(f"output ì „ì²´: {output}")

        # ì‘ë‹µ ì²˜ë¦¬ ê°œì„ 
        if hasattr(output, 'choices') and output.choices:
            result = output.choices[0].text.strip()
            # í† í° ì œí•œìœ¼ë¡œ ì˜ë ¸ëŠ”ì§€ í™•ì¸
            choice = output.choices[0]
            if hasattr(choice, 'finish_reason'):
                finish_reason = choice.finish_reason
                print(f"ìƒì„± ì¢…ë£Œ ì´ìœ : {finish_reason}")
                if finish_reason == 'length':
                    print(f"âš ï¸  í† í° ì œí•œ({max_tokens})ìœ¼ë¡œ ì‘ë‹µì´ ì˜ë ¸ìŠµë‹ˆë‹¤!")
        elif isinstance(output, dict) and 'choices' in output:
            result = output['choices'][0]['text'].strip()
            # í† í° ì œí•œìœ¼ë¡œ ì˜ë ¸ëŠ”ì§€ í™•ì¸ (dict í˜•íƒœ)
            choice = output['choices'][0]
            if 'finish_reason' in choice:
                finish_reason = choice['finish_reason']
                print(f"ìƒì„± ì¢…ë£Œ ì´ìœ : {finish_reason}")
                if finish_reason == 'length':
                    print(f"âš ï¸  í† í° ì œí•œ({max_tokens})ìœ¼ë¡œ ì‘ë‹µì´ ì˜ë ¸ìŠµë‹ˆë‹¤!")
        else:
            result = str(output).strip()

        # ì‘ë‹µ ê¸¸ì´ ì •ë³´ ì¶œë ¥
        print(f"ìƒì„±ëœ ì‘ë‹µ ê¸¸ì´: {len(result)}ì")
        
        # í† í° ì œí•œìœ¼ë¡œ ì˜ë¦° ê²½ìš° ì¬ì‹œë„ ë¡œì§
        was_truncated = False
        if hasattr(output, 'choices') and output.choices:
            choice = output.choices[0]
            if hasattr(choice, 'finish_reason') and choice.finish_reason == 'length':
                was_truncated = True
        elif isinstance(output, dict) and 'choices' in output:
            choice = output['choices'][0]
            if choice.get('finish_reason') == 'length':
                was_truncated = True
        
        # JSONì´ ì™„ì „í•˜ì§€ ì•Šì€ ê²½ìš°ë„ ì²´í¬
        if not was_truncated and result:
            # JSON ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
            if '```json' in result:
                json_block = result[result.find('```json'):]
                # JSONì´ ì œëŒ€ë¡œ ë‹«íˆì§€ ì•Šì€ ê²½ìš°
                if json_block.count('{') != json_block.count('}'):
                    print("âš ï¸  JSON ì¤‘ê´„í˜¸ê°€ ë§ì§€ ì•Šì•„ ì˜ë¦° ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.")
                    was_truncated = True
        
        # ì˜ë¦° ê²½ìš° í•œ ë²ˆ ë” ì‹œë„ (í† í° ìˆ˜ ì¦ê°€)
        if was_truncated and max_tokens < 1200:
            retry_max_tokens = max_tokens * 2
            print(f"ğŸ”„ í† í° ì œí•œìœ¼ë¡œ ì˜ë¦° ì‘ë‹µ ì¬ì‹œë„ (max_tokens: {max_tokens} â†’ {retry_max_tokens})")
            
            retry_output = llm(
                prompt,
                max_tokens=retry_max_tokens,
                temperature=0.3,
                min_p=0.1,
                top_p=0.8,
                top_k=20,
                repeat_penalty=1.05,
                echo=False
            )
            
            # ì¬ì‹œë„ ê²°ê³¼ ì²˜ë¦¬
            if hasattr(retry_output, 'choices') and retry_output.choices:
                retry_result = retry_output.choices[0].text.strip()
                print(f"ì¬ì‹œë„ ì‘ë‹µ ê¸¸ì´: {len(retry_result)}ì")
                
                # ì¬ì‹œë„ê°€ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸
                if len(retry_result) > len(result) and '```json' in retry_result:
                    print("âœ… ì¬ì‹œë„ ì„±ê³µ - ë” ì™„ì „í•œ ì‘ë‹µ íšë“")
                    result = retry_result
                    output = retry_output
            elif isinstance(retry_output, dict) and 'choices' in retry_output:
                retry_result = retry_output['choices'][0]['text'].strip()
                print(f"ì¬ì‹œë„ ì‘ë‹µ ê¸¸ì´: {len(retry_result)}ì")
                
                if len(retry_result) > len(result) and '```json' in retry_result:
                    print("âœ… ì¬ì‹œë„ ì„±ê³µ - ë” ì™„ì „í•œ ì‘ë‹µ íšë“")
                    result = retry_result
                    output = retry_output
        
        # ì›ë³¸ ì‘ë‹µì„ í•­ìƒ ëª…í™•íˆ ì¶œë ¥
        print(f"[ì›ë³¸ ì‘ë‹µ]:\n{result}\n---")
        log_gemma_response(result, "gemma_summarizer")


        
        # JSON ì¶”ì¶œ ë° ì²˜ë¦¬ (json_repair ëª¨ë“ˆ ì‚¬ìš©)
        json_str = extract_json_from_markdown(result)
        
        if json_str is None:
            # JSON ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            print("JSON ì¶”ì¶œ ì‹¤íŒ¨ - ì›ë³¸ ë°ì´í„° ì¶”ì¶œ ì‹œë„")
            extracted_data = extract_valid_data_from_broken_json(result)
            processed_result = ResponsePostprocessor.process_response(extracted_data)
            return json.dumps(processed_result, ensure_ascii=False, indent=2)
        
        # JSON ì²˜ë¦¬ ë° ë³µêµ¬
        final_json = process_and_repair_json(json_str)
        
        # ìµœì¢… í›„ì²˜ë¦¬
        try:
            parsed_result = json.loads(final_json)
            print(f"ğŸ” í›„ì²˜ë¦¬ ì „ parsed_result: {parsed_result}")
            processed_result = ResponsePostprocessor.process_response(parsed_result)
            print(f"ğŸ” í›„ì²˜ë¦¬ í›„ processed_result: {processed_result}")
            return json.dumps(processed_result, ensure_ascii=False, indent=2)
        except json.JSONDecodeError as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
            print(f"âŒ summarize_with_gemma JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ“„ ì‹¤íŒ¨í•œ JSON ì „ì²´ ë‚´ìš© (ê¸¸ì´: {len(final_json)} ë¬¸ì):")
            print("="*80)
            print(final_json)
            print("="*80)
            print(f"ğŸ” ì˜¤ë¥˜ ìœ„ì¹˜: ì¤„ {e.lineno}, ì»¬ëŸ¼ {e.colno}, ë¬¸ì {e.pos}")
            if e.pos < len(final_json):
                start = max(0, e.pos - 50)
                end = min(len(final_json), e.pos + 50)
                print(f"ğŸ¯ ì˜¤ë¥˜ ì£¼ë³€ ë‚´ìš©: '{final_json[start:end]}'")
                print(f"ğŸ¯ ì˜¤ë¥˜ ë¬¸ì: '{final_json[e.pos] if e.pos < len(final_json) else 'EOF'}'")
            
            # ìµœì¢… ì‹¤íŒ¨ ì‹œ ë¹ˆ êµ¬ì¡° ë°˜í™˜
            processed_result = ResponsePostprocessor.process_response({"summary": "", "keyword": "", "paragraphs": []})
            return json.dumps(processed_result, ensure_ascii=False, indent=2)
            
    except Exception as e:
        error_msg = f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        return json.dumps({"summary": "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, ensure_ascii=False)

def process_request(data: dict) -> dict:
    """
    ìš”ì²­ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        data (dict): ìš”ì²­ ë°ì´í„° (request_id, text í¬í•¨)

    Returns:
        dict: ìƒˆë¡œìš´ ì‘ë‹µ ê·œê²©ì— ë§ëŠ” ì‘ë‹µ ë°ì´í„°
    """
    try:
        # ìš”ì²­ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        transactionid = data.get("transactionid", "")
        sequenceno = data.get("sequenceno", "0")
        request_id = data.get("request_id", "unknown")
        text = data.get("text", "")

        if not text.strip():
            response_data = {
                "transactionid": transactionid,
                "sequenceno": sequenceno,
                "returncode": "1",
                "returndescription": "Success",
                "response": {
                    "result": "1",
                    "failReason": "ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                    "summary": ""
                }
            }
            return response_data

        print(f"ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ (ID: {request_id})")
        start_time = time.time()

        # ì²« ë²ˆì§¸ ìš”ì•½ ìˆ˜í–‰
        summary = summarize_with_gemma(text)
        
        # í›„ì²˜ë¦¬ ìˆ˜í–‰
        try:
            # summarize_with_gemma í•¨ìˆ˜ì—ì„œ ì´ë¯¸ 1ì°¨ í›„ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆìœ¼ë¯€ë¡œ,
            # ì—¬ê¸°ì„œëŠ” json íŒŒì‹±ë§Œ ìˆ˜í–‰í•˜ì—¬ ì¬ì§ˆì˜ í•„ìš” ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
            try:
                processed_response = json.loads(summary)
            except json.JSONDecodeError as e:
                print(f"âŒ process_request JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                print(f"ğŸ“„ ì‹¤íŒ¨í•œ JSON ì „ì²´ ë‚´ìš© (ê¸¸ì´: {len(summary)} ë¬¸ì):")
                print("="*80)
                print(summary)
                print("="*80)
                print(f"ğŸ” ì˜¤ë¥˜ ìœ„ì¹˜: ì¤„ {e.lineno}, ì»¬ëŸ¼ {e.colno}, ë¬¸ì {e.pos}")
                if e.pos < len(summary):
                    start = max(0, e.pos - 50)
                    end = min(len(summary), e.pos + 50)
                    print(f"ğŸ¯ ì˜¤ë¥˜ ì£¼ë³€ ë‚´ìš©: '{summary[start:end]}'")
                    print(f"ğŸ¯ ì˜¤ë¥˜ ë¬¸ì: '{summary[e.pos] if e.pos < len(summary) else 'EOF'}'")
                raise
            
            # ResponsePostprocessorë¡œ ìµœì¢… í›„ì²˜ë¦¬ ìˆ˜í–‰
            print(f"ğŸ” process_request í›„ì²˜ë¦¬ ì „: {processed_response}")
            processed_response = ResponsePostprocessor.process_response(processed_response)
            print(f"ğŸ” process_request í›„ì²˜ë¦¬ í›„: {processed_response}")
            
            processed_summary = processed_response.get('summary', '')
            
            # ì¬ì§ˆì˜ í•„ìš” ì—¬ë¶€ í™•ì¸
            if processed_summary.startswith('[ì¬ì§ˆì˜ í•„ìš”]'):
                # ì¬ì§ˆì˜ ë°œìƒ ë¡œê·¸ ê¸°ë¡
                original_length = len(processed_summary.replace('[ì¬ì§ˆì˜ í•„ìš”] ', ''))
                
                # ë¡œê·¸ íŒŒì¼ì— ì¬ì§ˆì˜ ë°œìƒ ê¸°ë¡
                log_gemma_query(f"ğŸ”„ ì¬ì§ˆì˜ í•„ìš” ê°ì§€: {processed_summary}", "requery_detection")
                log_gemma_query(f"ğŸ“ ì›ë³¸ ìš”ì•½ ê¸¸ì´: {original_length}ë°”ì´íŠ¸ (120ë°”ì´íŠ¸ ì´ˆê³¼)", "requery_detection")
                log_gemma_query(f"ğŸ“ ì¬ì§ˆì˜ ì´ìœ : ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ì••ì¶• ì¬ì§ˆì˜ í•„ìš”", "requery_detection")
                log_gemma_query(f"ì¬ì§ˆì˜ ì „ processed_response: {json.dumps(processed_response, ensure_ascii=False, indent=2)}", "requery_detection")
                log_gemma_query(f"ì¬ì§ˆì˜ ì „ keyword: {processed_response.get('keyword', 'ì—†ìŒ')}", "requery_detection")
                log_gemma_query(f"ì¬ì§ˆì˜ ì „ sentiment: {processed_response.get('sentiment', 'ì—†ìŒ')}", "requery_detection")
                
                # ì¬ì§ˆì˜ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (ì´ë¯¸ ì²˜ë¦¬ëœ summaryë¥¼ ì¬ì§ˆì˜)
                # [ì¬ì§ˆì˜ í•„ìš”] ë¬¸êµ¬ ì œê±°
                original_summary = processed_summary.replace('[ì¬ì§ˆì˜ í•„ìš”] ', '')
                requery_prompt = (
                    f"ë‹¤ìŒ ìš”ì•½ì„ ë§¤ìš° ì§§ì€ ìš”ì•½ìœ¼ë¡œ ë‹¤ì‹œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n"
                    f"ì˜ˆì‹œ:\n"
                    f"ì›ë³¸: ê¸°ì¡´ í‰ìƒ êµìœ¡ í¬ë§ ì¹´ë“œëŠ” ì‘ë…„ê¹Œì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥í–ˆê³  ë†í˜‘ ì²´í—˜ ì¹´ë“œë¥¼ ë°œê¸‰ë°›ì•„ì•¼ í¬ì¸íŠ¸ ì§€ê¸‰ ê°€ëŠ¥í•˜ì—¬ ì¹´ë“œ ë°œê¸‰ ë°©ë²•ì„ ì•ˆë‚´ë“œë ¸ìŠµë‹ˆë‹¤.\n"
                    f"ìš”ì•½: ë†í˜‘ ì¹´ë“œ ë°œê¸‰ ì•ˆë‚´\n\n"
                    f"ì›ë³¸ ìš”ì•½:\n{original_summary}\n\n"
                    f"ì¬ìš”ì•½:"
                )
                
                # ì¬ì§ˆì˜ ìˆ˜í–‰
                start_time = time.time()
                log_gemma_query(f"ğŸ”„ ì¬ì§ˆì˜ ì‹œì‘...", "requery_start")
                
                config = get_config()
                llm = get_llm_instance()
                
                # ì¬ì§ˆì˜ìš© max_tokens ì„¤ì • (ê¸°ë³¸ê°’ ì‚¬ìš©)
                requery_max_tokens = config.get('DEFAULT_MAX_TOKENS', 500)
                
                # ì¬ì§ˆì˜ ì‹œì‘ ë¡œê·¸
                log_gemma_query(requery_prompt, "requery_prompt")
                
                requery_response = llm(
                    requery_prompt,
                    max_tokens=requery_max_tokens,
                    temperature=0.3,  # ë§¤ìš° ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± ê·¹ëŒ€í™”
                    min_p=0.1,  # ë” ì—„ê²©í•œ ìµœì†Œ í™•ë¥ 
                    top_p=0.8,  # ë” ë‚®ì€ top_pë¡œ ì¼ê´€ì„± í–¥ìƒ
                    top_k=20,  # ë” ì¢ì€ í† í° ì„ íƒ ë²”ìœ„
                    repeat_penalty=1.05,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
                    echo=False
                )
                
                end_time = time.time()
                requery_time = end_time - start_time
                
                requery_summary = requery_response['choices'][0]['text'].strip()
                requery_length = len(requery_summary)
                
                # ì¬ì§ˆì˜ ì™„ë£Œ ë¡œê·¸
                log_gemma_response(f"âœ… ì¬ì§ˆì˜ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {requery_time:.2f}ì´ˆ)", "requery_result")
                log_gemma_response(f"ğŸ“ ì¬ì§ˆì˜ ê²°ê³¼ ê¸¸ì´: {requery_length}ë°”ì´íŠ¸", "requery_result")
                log_gemma_response(f"ğŸ“ ì¬ì§ˆì˜ ê²°ê³¼: {requery_summary}", "requery_result")
                log_gemma_response(f"ğŸ”„ ì••ì¶•ë¥ : {original_length}ë°”ì´íŠ¸ â†’ {requery_length}ë°”ì´íŠ¸ ({((original_length-requery_length)/original_length*100):.1f}% ë‹¨ì¶•)", "requery_result")
                
                # ì¬ì§ˆì˜ ê²°ê³¼ë¥¼ processed_responseì˜ summaryì— ì§ì ‘ ì„¤ì •
                # ê¸°ì¡´ processed_response êµ¬ì¡°ëŠ” ìœ ì§€í•˜ê³  summaryë§Œ ì—…ë°ì´íŠ¸
                # ì¬ì§ˆì˜ ì „í›„ ìƒíƒœ ë¡œê·¸
                log_gemma_response(f"ì¬ì§ˆì˜ ì „ processed_response íƒ€ì…: {type(processed_response)}", "requery_processing")
                log_gemma_response(f"ì¬ì§ˆì˜ ì „ processed_response í‚¤: {list(processed_response.keys())}", "requery_processing")
                log_gemma_response(f"ì¬ì§ˆì˜ ì „ processed_response ì „ì²´: {json.dumps(processed_response, ensure_ascii=False, indent=2)}", "requery_processing")
                
                # summaryë§Œ ì—…ë°ì´íŠ¸
                processed_response['summary'] = requery_summary
                
                log_gemma_response(f"ì¬ì§ˆì˜ í›„ processed_response: {json.dumps(processed_response, ensure_ascii=False, indent=2)}", "requery_processing")
                log_gemma_response(f"ì¬ì§ˆì˜ í›„ keyword: {processed_response.get('keyword', 'ì—†ìŒ')}", "requery_processing")
                log_gemma_response(f"ì¬ì§ˆì˜ í›„ sentiment: {processed_response.get('sentiment', 'ì—†ìŒ')}", "requery_processing")
                log_gemma_response(f"ì¬ì§ˆì˜ í›„ processed_response íƒ€ì…: {type(processed_response)}", "requery_processing")
                log_gemma_response(f"ì¬ì§ˆì˜ í›„ processed_response í‚¤: {list(processed_response.keys())}", "requery_processing")

                # ì¬ì§ˆì˜ í›„ì²˜ë¦¬ ìˆ˜í–‰ (ë‹¨, [ì¬ì§ˆì˜ í•„ìš”] íƒœê·¸ëŠ” ë‹¤ì‹œ ë¶™ì´ì§€ ì•ŠìŒ)
                log_gemma_response(f"ğŸ” process_request ì¬ì§ˆì˜ í›„ì²˜ë¦¬ ì „: {processed_response}", "requery_postprocess")
                
                # ì¬ì§ˆì˜ í›„ì—ëŠ” convert_to_noun_formë§Œ ì ìš©í•˜ê³  [ì¬ì§ˆì˜ í•„ìš”] íƒœê·¸ëŠ” ë¶™ì´ì§€ ì•ŠìŒ
                if 'summary' in processed_response:
                    original_summary_before_noun = processed_response['summary']
                    # convert_to_noun_formë§Œ ì ìš© (ê¸¸ì´ ì²´í¬ ì—†ì´)
                    processed_summary = ResponsePostprocessor.convert_to_noun_form(original_summary_before_noun)
                    processed_response['summary'] = processed_summary
                    log_gemma_response(f"ğŸ” ì¬ì§ˆì˜ í›„ ëª…ì‚¬í˜• ë³€í™˜: '{original_summary_before_noun}' â†’ '{processed_summary}'", "requery_postprocess")
                
                final_length = len(processed_response.get('summary', ''))
                log_gemma_response(f"ğŸ” process_request ì¬ì§ˆì˜ í›„ì²˜ë¦¬ í›„: {processed_response}", "requery_postprocess")
                log_gemma_response(f"ğŸ¯ ì¬ì§ˆì˜ ì „ì²´ ê³¼ì • ì™„ë£Œ: ìµœì¢… ìš”ì•½ ê¸¸ì´ {final_length}ë°”ì´íŠ¸", "requery_complete")
                
                # ì¬ì§ˆì˜ ì „ì²´ ê³¼ì • ì™„ë£Œ ë¡œê·¸
                log_gemma_response(f"[ì¬ì§ˆì˜ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ] ìµœì¢… ìš”ì•½: {processed_response.get('summary', '')}, ìµœì¢… ê¸¸ì´: {final_length}ë°”ì´íŠ¸", "requery_process_complete")

            # ìµœì¢… ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì‚¬ìš©
            # processed_responseëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            summary = processed_response
                
        except json.JSONDecodeError:
            print("ì›ë³¸ ìš”ì•½ JSON íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸ ì‚¬ìš©")
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
            pass

        processing_time = time.time() - start_time
        
        print(f"ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ (ID: {request_id}, ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")

        # ì„±ê³µ ì‘ë‹µ
        response_data = {
            "transactionid": transactionid,
            "sequenceno": sequenceno,
            "returncode": "1",
            "returndescription": "Success",
            "response": {
                "result": "0",
                "failReason": "",
                "summary": summary
            }
        }

        return response_data

    except Exception as e:
        error_msg = f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(error_msg)

        # ì˜¤ë¥˜ ì‘ë‹µ
        response_data = {
            "transactionid": data.get("transactionid", ""),
            "sequenceno": data.get("sequenceno", "0"),
            "returncode": "1",
            "returndescription": "Success",
            "response": {
                "result": "1",
                "failReason": error_msg,
                "summary": ""
            }
        }

        return response_data

# ë‹¨ë… ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("=== Gemma Summarizer ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ===")

    # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
    if not validate_config():
        print("ì„¤ì • ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. config.pyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ì¬ì§ˆì˜ ë°œìƒì„ ìœ„í•œ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸)
    test_text = """
    ê¹€ë¯¼ì¤€ íŒ€ì¥: ì—¬ë³´ì„¸ìš”, ì´ì„œì—° ëŒ€ë¦¬ë‹˜. ê¹€ë¯¼ì¤€ íŒ€ì¥ì…ë‹ˆë‹¤.

ì´ì„œì—° ëŒ€ë¦¬: ë„¤, íŒ€ì¥ë‹˜! ì•ˆë…•í•˜ì„¸ìš”. ì „í™” ì£¼ì…¨ë„¤ìš”.

ê¹€ë¯¼ì¤€ íŒ€ì¥: ë„¤, ë‹¤ë¦„ì´ ì•„ë‹ˆë¼ ë‹¤ìŒ ì£¼ ìˆ˜ìš”ì¼ë¡œ ì˜ˆì •ëœ 'ì•ŒíŒŒ í”„ë¡œì íŠ¸' ì‹ ì œí’ˆ ëŸ°ì¹­ ìº í˜ì¸ ê´€ë ¨í•´ì„œ ìµœì¢… ì§„í–‰ ìƒí™© ì¢€ ì²´í¬í•˜ë ¤ê³  ì „í™”í–ˆì–´ìš”. ì¤€ë¹„ëŠ” ì˜ ë˜ì–´ê°€ê³  ìˆì£ ?

ì´ì„œì—° ëŒ€ë¦¬: ì•„, ë„¤. ë§ˆì¹¨ ì €ë„ ì¤‘ê°„ ë³´ê³  ë“œë¦¬ë ¤ê³  í–ˆìŠµë‹ˆë‹¤. ë¨¼ì €, SNS ê´‘ê³  ì†Œì¬ëŠ” ì–´ì œ ë””ìì¸íŒ€ì—ì„œ ì‹œì•ˆ 2ê°œë¥¼ ë°›ì•˜ê³ , ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ì œê°€ ìµœì¢… 1ê°œ ì„ íƒí•´ì„œ ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¸í”Œë£¨ì–¸ì„œ í˜‘ì—…ì€ ì´ 5ëª…ê³¼ ê³„ì•½ì´ ì™„ë£Œë˜ì—ˆê³ , ê°ê°ì˜ ì½˜í…ì¸  ê¸°íšì•ˆë„ ìŠ¹ì¸ë°›ì•˜ìŠµë‹ˆë‹¤. ë˜í•œ ì˜¤í”„ë¼ì¸ ì´ë²¤íŠ¸ëŠ” ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ë¶€í„° ëª©ìš”ì¼ê¹Œì§€ 4ì¼ê°„ ê°•ë‚¨ì—­, í™ëŒ€ì…êµ¬, ë¶€ì‚° ì„œë©´ì—ì„œ ì§„í–‰í•  ì˜ˆì •ì´ê³ , í˜„ì¥ ìŠ¤íƒœí”„ 20ëª…ë„ ëª¨ë‘ í™•ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë””ì§€í„¸ ë§ˆì¼€íŒ… ì˜ˆì‚°ì€ ì´ 5ì²œë§Œì›ìœ¼ë¡œ ì„¤ì •í–ˆê³ , í˜ì´ìŠ¤ë¶, ì¸ìŠ¤íƒ€ê·¸ë¨, ìœ íŠœë¸Œ ì±„ë„ì— ê³¨ê³ ë£¨ ë°°ë¶„í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¶”ê°€ë¡œ ê³ ê° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ë„ ì™„ë£Œë˜ì—ˆê³ , CRM ì‹œìŠ¤í…œ ì—°ë™ë„ ëë‚¬ìŠµë‹ˆë‹¤. ë§ˆì¼€íŒ… ìë™í™” íˆ´ë„ ì„¤ì • ì™„ë£Œí–ˆê³ , A/B í…ŒìŠ¤íŠ¸ ê³„íšë„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤. ì†Œì…œë¯¸ë””ì–´ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œë„ êµ¬ì¶•í–ˆê³ , ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œë„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ê³ ê° í”¼ë“œë°± ìˆ˜ì§‘ ì‹œìŠ¤í…œë„ êµ¬ì¶•í–ˆê³ , ë¶„ì„ ë¦¬í¬íŠ¸ í…œí”Œë¦¿ë„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¶”ê°€ë¡œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë§ˆì¼€íŒ…ë„ ì§„í–‰í•  ì˜ˆì •ì´ê³ , ì¹´ì¹´ì˜¤í†¡ ì±„ë„ë„ ê°œì„¤í•´ì„œ ê³ ê°ê³¼ì˜ ì†Œí†µ ì±„ë„ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ PR í™œë™ë„ ì¤€ë¹„ ì¤‘ì´ë©°, ì£¼ìš” IT ë§¤ì²´ë“¤ê³¼ ì¸í„°ë·° ì¼ì •ë„ ì¡°ìœ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ê³ ê° ë§Œì¡±ë„ ì¡°ì‚¬ ì‹œìŠ¤í…œë„ êµ¬ì¶•í–ˆê³ , ë¦¬ë·° ê´€ë¦¬ ì‹œìŠ¤í…œë„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ë§ˆì¼€íŒ… ì„±ê³¼ ì¸¡ì • ì§€í‘œë„ ì„¤ì •í–ˆê³ , ROI ë¶„ì„ ë„êµ¬ë„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.
    """

    # ìš”ì•½ í…ŒìŠ¤íŠ¸
    result = summarize_with_gemma(test_text)
    print(f"\n=== ìš”ì•½ ê²°ê³¼ ===")
    print(result)